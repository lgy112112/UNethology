{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该如何加载这些数据呢？首先我们肯定能确认：图像和它对应的掩码肯定是成对出现的。\n",
    "\n",
    "按照我的习惯，我会事先将所有图像和掩码的路径都写在一个metadata.csv中，这样可以方便地读取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以展开下方的cell运行，得到metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.csv 已保存到: lgg-mri-segmentation/kaggle_3m/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 指定根文件夹路径\n",
    "root_folder = 'lgg-mri-segmentation/kaggle_3m'\n",
    "\n",
    "# 列表用于存储image和mask路径\n",
    "data = {'image_path': [], 'mask_path': []}\n",
    "\n",
    "# 遍历根文件夹下的所有子文件夹\n",
    "for sub_folder in os.listdir(root_folder):\n",
    "    sub_folder_path = os.path.join(root_folder, sub_folder)\n",
    "\n",
    "    # 检查是否为文件夹\n",
    "    if os.path.isdir(sub_folder_path):\n",
    "        # 遍历子文件夹中的所有文件\n",
    "        for file_name in os.listdir(sub_folder_path):\n",
    "            if file_name.endswith('.tif') and '_mask' not in file_name:\n",
    "                # 获取切片路径\n",
    "                slice_path = os.path.join(sub_folder_path, file_name)\n",
    "\n",
    "                # 获取对应的掩膜文件路径\n",
    "                base_name = file_name.replace('.tif', '')\n",
    "                mask_file_name = base_name + '_mask.tif'\n",
    "                mask_path = os.path.join(sub_folder_path, mask_file_name)\n",
    "                \n",
    "                # 仅当掩膜文件存在时，记录图像和掩膜路径\n",
    "                if os.path.exists(mask_path):\n",
    "                    data['image_path'].append(slice_path)\n",
    "                    data['mask_path'].append(mask_path)\n",
    "\n",
    "# 创建DataFrame并写入CSV文件\n",
    "df = pd.DataFrame(data)\n",
    "csv_output_path = os.path.join(root_folder, 'metadata.csv')\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"metadata.csv 已保存到: {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是我们得到了[lgg-mri-segmentation/kaggle_3m/metadata.csv](lgg-mri-segmentation/kaggle_3m/metadata.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们构建dataset和dataloader\n",
    "\n",
    "他们有什么不同呢？Personally，dataset是一个将原始文件进行汇集的`东西`，可以说，dataset里获得到的文件还是原来的那个样子，因为dataset不过是个中介而已：作用是负责将原始数据组织成可用于模型训练和验证的格式。它不会处理批量加载或打乱顺序等操作，只是单独地获取数据样本。\n",
    "\n",
    "而dataloader则是中介之后，将文件输送给模型的`传送带`了，你知道，模型只看得懂张量，dataloader就负责将我们手头上的文件变成张量，然后通过迭代访问送给模型。\n",
    "\n",
    "当然，你也可以一次性在一个dataloader里完成所有这些任务，都无所谓，无非形式之变。\n",
    "\n",
    "我在此使用我最喜欢的`pytoch lightning`中的dataloader变体`datamodule`外加dataset进行示范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "\n",
    "# 自定义 PyTorch Dataset\n",
    "class BrainLesionDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx]['image_path']\n",
    "        mask_path = self.data.iloc[idx]['mask_path']\n",
    "        \n",
    "        # 打开图像和掩膜\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        mask = Image.open(mask_path).convert('L')  # 掩膜为灰度图像\n",
    "\n",
    "        # 如果有transform，应用到图像和掩膜\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask.long() # 强调一下要整数不要浮点数\n",
    "\n",
    "# PyTorch Lightning DataModule\n",
    "class BrainLesionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, csv_file, batch_size=16, num_workers=4, transform=None, split_ratio=(0.6, 0.2, 0.2)):\n",
    "        super().__init__()\n",
    "        self.csv_file = csv_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transform\n",
    "        self.split_ratio = split_ratio  # 训练、验证和测试集的划分比例\n",
    "\n",
    "    # 准备数据集\n",
    "    def setup(self, stage=None):\n",
    "        # 创建完整的数据集\n",
    "        full_dataset = BrainLesionDataset(self.csv_file, transform=self.transform)\n",
    "\n",
    "        # 计算每个数据集的大小\n",
    "        dataset_size = len(full_dataset)\n",
    "        train_size = int(self.split_ratio[0] * dataset_size)\n",
    "        val_size = int(self.split_ratio[1] * dataset_size)\n",
    "        test_size = dataset_size - train_size - val_size\n",
    "\n",
    "        # 使用 random_split 进行数据集划分\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
    "\n",
    "# 示例的 transform，可以根据任务要求更改\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)), # 因为已经是256规格，所以在这里我不进行resize，实际上你可以进行128的resize，但请注意resize方法，掩码只适合使用最近邻插值的resize方法\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照Pytorch Lightning的使用规范，我们可以实例化出train_loader/val_loader/test_loader，然后可以检查里面的东西。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([16, 1, 256, 256]), Masks shape: torch.Size([16, 1, 256, 256])\n",
      "Image min&Max: (tensor(0.), tensor(0.9490)), Masks unique: [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "num_workers = os.cpu_count()\n",
    "csv_file = 'lgg-mri-segmentation/kaggle_3m/metadata.csv'\n",
    "# 定义 DataModule，使用自动获得的 num_workers\n",
    "data_module = BrainLesionDataModule(csv_file, batch_size=16, num_workers=num_workers, transform=transform)\n",
    "\n",
    "# 检查 DataModule 是否正常工作\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "\n",
    "# 打印第一个 batch 的形状\n",
    "for batch in train_loader:\n",
    "    images, masks = batch\n",
    "    print(f\"Images shape: {images.shape}, Masks shape: {masks.shape}\")\n",
    "    print(f\"Image min&Max: {images.min(), images.max()}, Masks unique: {np.unique(masks)}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到图像最值和掩码独特值变成了0/1之间，这应该是ToTensor造成的，他会自己识别并normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型怎么办？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备好了数据，只欠东风了，那么我如何准备模型？\n",
    "\n",
    "按我的习惯，就两条路。\n",
    "\n",
    "1. 你已经有了准备好的模型，那么你需要调查一下这个模型的输入输出是多少，如果你的模型只接受512x512的输入，那么我们目前的256x256的规格就不符合了。你就需要对数据进行resize或者对模型进行修改\n",
    "\n",
    "2. 你没有准备好的数据，你准备进行freestyle，那么就无所谓了，只需要注意你构建的模型需要契合当前数据的维度\n",
    "\n",
    "在此我们看到`Images shape: torch.Size([8, 3, 256, 256]), Masks shape: torch.Size([8, 1, 256, 256])`，很容易知道模型的输入要与`Images shape: torch.Size([8, 3, 256, 256])`契合。\n",
    "\n",
    "那么如果我们进行分割任务，模型的输出势必也要和`Masks shape: torch.Size([8, 1, 256, 256])`相等\n",
    "\n",
    "这样才能进行对比和loss的计算，不然鸡同鸭讲，成何体统呢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们使用UNet进行这个任务；我在此使用来自MONAI的UNet，你应该要用开发者方式来安装它，这样我们就能在日后方便地更改里面的代码。\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/Project-MONAI/MONAI.git\n",
    "\n",
    "cd MONAI/\n",
    "\n",
    "python setup.py develop\n",
    "```\n",
    "\n",
    "我们将要使用**BasicUNet**做一个benchmark，你可以在这里找到它：[BasicUNet](MONAI/monai/networks/nets/basic_unet.py)\n",
    "\n",
    "你可以看到**BasicUNet的定义**中有这么几个参数是我们需要设置的：\n",
    "\n",
    "```python\n",
    "        spatial_dims: int = 3,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 2,\n",
    "        features: Sequence[int] = (32, 32, 64, 128, 256, 32),\n",
    "        act: str | tuple = (\"LeakyReLU\", {\"negative_slope\": 0.1, \"inplace\": True}),\n",
    "        norm: str | tuple = (\"instance\", {\"affine\": True}),\n",
    "        bias: bool = True,\n",
    "        dropout: float | tuple = 0.0,\n",
    "        upsample: str = \"deconv\",\n",
    "```\n",
    "\n",
    "其中act、norm、bias、upsample我们可以不管，但其它的东西我们要稍后在定义模型时声明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确认好了模型，我们构建训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicUNet features: (32, 32, 64, 128, 256, 32).\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from monai.losses import DiceCELoss\n",
    "# !pip install -U torchmetrics # fuck torchmetrics终于加入了分割用的Dice\n",
    "# from torchmetrics.segmentation import GeneralizedDiceScore # https://lightning.ai/docs/torchmetrics/stable/segmentation/generalized_dice.html\n",
    "from monai.networks.nets import BasicUNet # 在此，我们将BasicUNet导入进来\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import ToPILImage\n",
    "to_pil = ToPILImage()\n",
    "\n",
    "class BasicUNetLightning(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = BasicUNet(\n",
    "            spatial_dims=2,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            features=(32,32,64,128,256,32),\n",
    "            dropout=0.50,\n",
    "        )\n",
    "        self.loss = DiceCELoss()\n",
    "        self.dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.model(x))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.loss(outputs, masks)\n",
    "        \n",
    "        # 对模型输出进行阈值化处理\n",
    "        thresholded_outputs = (outputs > 0.5).float()\n",
    "        \n",
    "        # 计算 Dice 系数\n",
    "        self.dice_metric(y_pred=thresholded_outputs, y=masks)\n",
    "        dice_score = self.dice_metric.aggregate().item()\n",
    "        self.dice_metric.reset()\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"train_dice\", dice_score, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.loss(outputs, masks)\n",
    "        \n",
    "        # 对模型输出进行阈值化处理\n",
    "        thresholded_outputs = (outputs > 0.5).float()\n",
    "        \n",
    "        # 计算 Dice 系数\n",
    "        self.dice_metric(y_pred=thresholded_outputs, y=masks)\n",
    "        dice_score = self.dice_metric.aggregate().item()\n",
    "        self.dice_metric.reset()\n",
    "        \n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_dice\", dice_score, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        \n",
    "        return loss  # 如果需要返回损失\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.loss(outputs, masks)\n",
    "        \n",
    "        # 对模型输出进行阈值化处理\n",
    "        thresholded_outputs = (outputs > 0.5).float()\n",
    "        \n",
    "        # 计算 Dice 系数\n",
    "        self.dice_metric(y_pred=thresholded_outputs, y=masks)\n",
    "        dice_score = self.dice_metric.aggregate().item()\n",
    "        self.dice_metric.reset()\n",
    "        \n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"test_dice\", dice_score, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        \n",
    "        return loss  # 如果需要返回损失\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        os.makedirs(\"predictions\", exist_ok=True)\n",
    "        for i in range(images.size(0)):\n",
    "            image = to_pil(images[i])\n",
    "            mask = to_pil(masks[i].float())\n",
    "            output = to_pil(outputs[i].float())\n",
    "            combined_image = Image.new(\"RGB\", (image.width * 3, image.height))\n",
    "            combined_image.paste(image, (0, 0))       # 左边是原图\n",
    "            combined_image.paste(mask, (image.width, 0)) # 中间是真掩码\n",
    "            combined_image.paste(output, (image.width * 2, 0)) # 右边是预测掩码\n",
    "            combined_image.save(f\"predictions/prediction_{batch_idx}_{i}.png\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "########################################################################################\n",
    "import os\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 定义 CSVLogger 的保存目录\n",
    "save_dir = \"UNet_logs\"\n",
    "logger = CSVLogger(save_dir=save_dir, name=\"UNet\")\n",
    "\n",
    "# 获取 CSVLogger 生成的日志文件夹路径\n",
    "checkpoint_dir = logger.log_dir\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 定义检查点回调，将模型保存在 CSVLogger 日志目录中\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpoint_dir,                       # 使用 CSVLogger 的日志目录\n",
    "    filename=\"UNet-{epoch:02d}-{val_dice:.4f}\",   # 模型文件名格式化\n",
    "    monitor=\"val_dice\",                           # 监控的指标\n",
    "    mode=\"max\",                                   # 最大化监控指标\n",
    "    save_top_k=1,                                 # 仅保存最佳模型\n",
    "    save_last=True                                # 保存最后一个模型\n",
    ")\n",
    "\n",
    "# 定义 Trainer，添加自定义的回调\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=150, # 你可以调到100\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    # overfit_batches=10,\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model = BasicUNetLightning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA L40S') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | BasicUNet  | 2.0 M  | train\n",
      "1 | loss  | DiceCELoss | 0      | train\n",
      "---------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "7.914     Total estimated model params size (MB)\n",
      "143       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00c2a8e8c2d477394b08ad9c5a2cf6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c793c84a16c4948a6d35a1b0b138279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81627579ce240b781ee7db3f2d38ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5ebbae9a12447a9310f67c382f1793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75f018fba2d47f4b7d20a4eabf037e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c65aa1c043c45489d855222db3d5311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bd1e434adc4cd3aa4b8f4b84fbe0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c827bee5af0409e85f9e97b575153d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc029850c3744f6a9f2dac05d3644da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, \n",
    "            data_module, \n",
    "            # ckpt_path='UNet_logs/UNet/version_29/UNet-epoch=46-val_dice=0.2358.ckpt',\n",
    "            )\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d101cee5dc474189a92a6f752b3e79f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/loops/prediction_loop.py:255: predict returned None if it was on purpose, ignore this warning...\n"
     ]
    }
   ],
   "source": [
    "test_loader = data_module.test_dataloader()\n",
    "prediction = trainer.predict(model, dataloaders=test_loader)\n",
    "# len(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
