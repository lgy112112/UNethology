{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该如何加载这些数据呢？首先我们肯定能确认：图像和它对应的掩码肯定是成对出现的。\n",
    "\n",
    "按照我的习惯，我会事先将所有图像和掩码的路径都写在一个metadata.csv中，这样可以方便地读取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以展开下方的cell运行，得到metadata.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.csv 已保存到: lgg-mri-segmentation/kaggle_3m/metadata.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 指定根文件夹路径\n",
    "root_folder = 'lgg-mri-segmentation/kaggle_3m'\n",
    "\n",
    "# 列表用于存储image和mask路径\n",
    "data = {'image_path': [], 'mask_path': []}\n",
    "\n",
    "# 遍历根文件夹下的所有子文件夹\n",
    "for sub_folder in os.listdir(root_folder):\n",
    "    sub_folder_path = os.path.join(root_folder, sub_folder)\n",
    "\n",
    "    # 检查是否为文件夹\n",
    "    if os.path.isdir(sub_folder_path):\n",
    "        # 遍历子文件夹中的所有文件\n",
    "        for file_name in os.listdir(sub_folder_path):\n",
    "            if file_name.endswith('.tif') and '_mask' not in file_name:\n",
    "                # 获取切片路径\n",
    "                slice_path = os.path.join(sub_folder_path, file_name)\n",
    "\n",
    "                # 获取对应的掩膜文件路径\n",
    "                base_name = file_name.replace('.tif', '')\n",
    "                mask_file_name = base_name + '_mask.tif'\n",
    "                mask_path = os.path.join(sub_folder_path, mask_file_name)\n",
    "                \n",
    "                # 仅当掩膜文件存在时，记录图像和掩膜路径\n",
    "                if os.path.exists(mask_path):\n",
    "                    data['image_path'].append(slice_path)\n",
    "                    data['mask_path'].append(mask_path)\n",
    "\n",
    "# 创建DataFrame并写入CSV文件\n",
    "df = pd.DataFrame(data)\n",
    "csv_output_path = os.path.join(root_folder, 'metadata.csv')\n",
    "df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "print(f\"metadata.csv 已保存到: {csv_output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "于是我们得到了[lgg-mri-segmentation/kaggle_3m/metadata.csv](lgg-mri-segmentation/kaggle_3m/metadata.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们构建dataset和dataloader\n",
    "\n",
    "他们有什么不同呢？Personally，dataset是一个将原始文件进行汇集的`东西`，可以说，dataset里获得到的文件还是原来的那个样子，因为dataset不过是个中介而已：作用是负责将原始数据组织成可用于模型训练和验证的格式。它不会处理批量加载或打乱顺序等操作，只是单独地获取数据样本。\n",
    "\n",
    "而dataloader则是中介之后，将文件输送给模型的`传送带`了，你知道，模型只看得懂张量，dataloader就负责将我们手头上的文件变成张量，然后通过迭代访问送给模型。\n",
    "\n",
    "当然，你也可以一次性在一个dataloader里完成所有这些任务，都无所谓，无非形式之变。\n",
    "\n",
    "我在此使用我最喜欢的`pytoch lightning`中的dataloader变体`datamodule`外加dataset进行示范。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "from torchvision import transforms\n",
    "\n",
    "# 自定义 PyTorch Dataset\n",
    "class BrainLesionDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data.iloc[idx]['image_path']\n",
    "        mask_path = self.data.iloc[idx]['mask_path']\n",
    "        \n",
    "        # 打开图像和掩膜\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        mask = Image.open(mask_path).convert('L')  # 掩膜为灰度图像\n",
    "\n",
    "        # 如果有transform，应用到图像和掩膜\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask.long() # 强调一下要整数不要浮点数\n",
    "\n",
    "# PyTorch Lightning DataModule\n",
    "class BrainLesionDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, csv_file, batch_size=16, num_workers=4, transform=None, split_ratio=(0.6, 0.2, 0.2)):\n",
    "        super().__init__()\n",
    "        self.csv_file = csv_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transform\n",
    "        self.split_ratio = split_ratio  # 训练、验证和测试集的划分比例\n",
    "\n",
    "    # 准备数据集\n",
    "    def setup(self, stage=None):\n",
    "        # 创建完整的数据集\n",
    "        full_dataset = BrainLesionDataset(self.csv_file, transform=self.transform)\n",
    "\n",
    "        # 计算每个数据集的大小\n",
    "        dataset_size = len(full_dataset)\n",
    "        train_size = int(self.split_ratio[0] * dataset_size)\n",
    "        val_size = int(self.split_ratio[1] * dataset_size)\n",
    "        test_size = dataset_size - train_size - val_size\n",
    "\n",
    "        # 使用 random_split 进行数据集划分\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n",
    "\n",
    "# 示例的 transform，可以根据任务要求更改\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)), # 因为已经是256规格，所以在这里我不进行resize，实际上你可以进行128的resize，但请注意resize方法，掩码只适合使用最近邻插值的resize方法\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照Pytorch Lightning的使用规范，我们可以实例化出train_loader/val_loader/test_loader，然后可以检查里面的东西。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([16, 1, 256, 256]), Masks shape: torch.Size([16, 1, 256, 256])\n",
      "Image min&Max: (tensor(0.), tensor(0.9725)), Masks unique: [0 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "num_workers = os.cpu_count()\n",
    "csv_file = 'lgg-mri-segmentation/kaggle_3m/metadata.csv'\n",
    "# 定义 DataModule，使用自动获得的 num_workers\n",
    "data_module = BrainLesionDataModule(csv_file, batch_size=16, num_workers=num_workers, transform=transform)\n",
    "\n",
    "# 检查 DataModule 是否正常工作\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "\n",
    "# 打印第一个 batch 的形状\n",
    "for batch in train_loader:\n",
    "    images, masks = batch\n",
    "    print(f\"Images shape: {images.shape}, Masks shape: {masks.shape}\")\n",
    "    print(f\"Image min&Max: {images.min(), images.max()}, Masks unique: {np.unique(masks)}\")\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到图像最值和掩码独特值变成了0/1之间，这应该是ToTensor造成的，他会自己识别并normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型怎么办？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备好了数据，只欠东风了，那么我如何准备模型？\n",
    "\n",
    "按我的习惯，就两条路。\n",
    "\n",
    "1. 你已经有了准备好的模型，那么你需要调查一下这个模型的输入输出是多少，如果你的模型只接受512x512的输入，那么我们目前的256x256的规格就不符合了。你就需要对数据进行resize或者对模型进行修改\n",
    "\n",
    "2. 你没有准备好的数据，你准备进行freestyle，那么就无所谓了，只需要注意你构建的模型需要契合当前数据的维度\n",
    "\n",
    "在此我们看到`Images shape: torch.Size([8, 3, 256, 256]), Masks shape: torch.Size([8, 1, 256, 256])`，很容易知道模型的输入要与`Images shape: torch.Size([8, 3, 256, 256])`契合。\n",
    "\n",
    "那么如果我们进行分割任务，模型的输出势必也要和`Masks shape: torch.Size([8, 1, 256, 256])`相等\n",
    "\n",
    "这样才能进行对比和loss的计算，不然鸡同鸭讲，成何体统呢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们使用UNet进行这个任务；我在此使用来自MONAI的UNet，你应该要用开发者方式来安装它，这样我们就能在日后方便地更改里面的代码。\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/Project-MONAI/MONAI.git\n",
    "\n",
    "cd MONAI/\n",
    "\n",
    "python setup.py develop\n",
    "```\n",
    "\n",
    "我们将要使用**BasicUNet**做一个benchmark，你可以在这里找到它：[BasicUNet](MONAI/monai/networks/nets/basic_unet.py)\n",
    "\n",
    "你可以看到**BasicUNet的定义**中有这么几个参数是我们需要设置的：\n",
    "\n",
    "```python\n",
    "        spatial_dims: int = 3,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 2,\n",
    "        features: Sequence[int] = (32, 32, 64, 128, 256, 32),\n",
    "        act: str | tuple = (\"LeakyReLU\", {\"negative_slope\": 0.1, \"inplace\": True}),\n",
    "        norm: str | tuple = (\"instance\", {\"affine\": True}),\n",
    "        bias: bool = True,\n",
    "        dropout: float | tuple = 0.0,\n",
    "        upsample: str = \"deconv\",\n",
    "```\n",
    "\n",
    "其中act、norm、bias、upsample我们可以不管，但其它的东西我们要稍后在定义模型时声明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确认好了模型，我们构建训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicUNet features: (32, 32, 64, 128, 256, 32).\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from monai.losses import DiceCELoss\n",
    "# !pip install -U torchmetrics # fuck torchmetrics终于加入了分割用的Dice\n",
    "from torchmetrics.segmentation import GeneralizedDiceScore # https://lightning.ai/docs/torchmetrics/stable/segmentation/generalized_dice.html\n",
    "from monai.networks.nets import BasicUNet # 在此，我们将BasicUNet导入进来\n",
    "import torchvision\n",
    "from torchvision.transforms import ToPILImage\n",
    "to_pil = ToPILImage()\n",
    "\n",
    "class BasicUNetLightning(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = BasicUNet(\n",
    "            spatial_dims=2,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            features=(32,32,64,128,256,32),\n",
    "            dropout=0.50,\n",
    "        )\n",
    "        self.loss = DiceCELoss() \n",
    "        # 也许我们要决定DiceCELoss要不要使用sigmoid，这其实取决于模型的输出和分割的任务\n",
    "        # 我们可以在basic_unet.py中找到模型的return为logits，并且我们执行的是脑肿瘤二分类\n",
    "        # 这意味着不是肿瘤就是背景，两者互斥，因此使用sigmoid\n",
    "\n",
    "        self.Dice = GeneralizedDiceScore(num_classes=2, include_background=False) #我们只有两个类别，并且我们只需要关注病变的Dice即可\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.model(x))\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        \n",
    "        # 直接用 logits 计算损失，因为这里的DiceCELoss会事先给我们进行sigmoid\n",
    "        loss = self.loss(outputs, masks)\n",
    "        \n",
    "        # 阈值化输出以计算 Dice，因为这里的self.Dice(preds, masks)可不会实现进行sigmoid\n",
    "        # preds = (torch.sigmoid(outputs) > 0.5).long()\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"train_dice\", self.Dice(outputs, masks), on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        \n",
    "        # 直接用 logits 计算损失，因为这里的DiceCELoss会事先给我们进行sigmoid\n",
    "        loss = self.loss(outputs, masks)\n",
    "        \n",
    "        # 阈值化输出以计算 Dice，因为这里的self.Dice(preds, masks)可不会实现进行sigmoid\n",
    "        # preds = (torch.sigmoid(outputs) > 0.5).long() \n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"val_dice\", self.Dice(outputs, masks), on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        \n",
    "        # 直接用 logits 计算损失，因为这里的DiceCELoss会事先给我们进行sigmoid\n",
    "        loss = self.loss(outputs, masks)\n",
    "        \n",
    "        # 阈值化输出以计算 Dice，因为这里的self.Dice(preds, masks)可不会实现进行sigmoid\n",
    "        # preds = (torch.sigmoid(outputs) > 0.5).long() \n",
    "        self.log(\"test_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "        self.log(\"test_dice\", self.Dice(outputs, masks), on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        images, masks = batch\n",
    "        outputs = self(images)\n",
    "        # print(images.shape, masks.shape, outputs.shape)\n",
    "        # preds = (torch.sigmoid(outputs) > 0.5).long()\n",
    "        # print(preds.shape)\n",
    "        os.makedirs(\"predictions\", exist_ok=True)\n",
    "        # print(images.size())\n",
    "        for i in range(images.size(0)):\n",
    "            image = to_pil(images[i])\n",
    "            mask = to_pil(masks[i].float())\n",
    "            output = to_pil(outputs[i].float())\n",
    "            # 接下来将image, mask, pred并排保存\n",
    "            combined_image = Image.new(\"RGB\", (image.width * 3, image.height))\n",
    "            combined_image.paste(image, (0, 0))       # 左边是原图\n",
    "            combined_image.paste(mask, (image.width, 0)) # 中间是真掩码\n",
    "            combined_image.paste(output, (image.width * 2, 0)) # 右边是预测掩码\n",
    "\n",
    "            # 保存拼接后的图像\n",
    "            combined_image.save(f\"predictions/prediction_{batch_idx}_{i}.png\")\n",
    "            # print(f\"predictions/prediction_{batch_idx}_{i}.png saved\")\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "########################################################################################\n",
    "import os\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# 定义 CSVLogger 的保存目录\n",
    "save_dir = \"UNet_logs\"\n",
    "logger = CSVLogger(save_dir=save_dir, name=\"UNet\")\n",
    "\n",
    "# 获取 CSVLogger 生成的日志文件夹路径\n",
    "checkpoint_dir = logger.log_dir\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# 定义检查点回调，将模型保存在 CSVLogger 日志目录中\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpoint_dir,                       # 使用 CSVLogger 的日志目录\n",
    "    filename=\"UNet-{epoch:02d}-{val_dice:.4f}\",   # 模型文件名格式化\n",
    "    monitor=\"val_dice\",                           # 监控的指标\n",
    "    mode=\"max\",                                   # 最大化监控指标\n",
    "    save_top_k=1,                                 # 仅保存最佳模型\n",
    "    save_last=True                                # 保存最后一个模型\n",
    ")\n",
    "\n",
    "# 定义 Trainer，添加自定义的回调\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50, # 你可以调到100\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    # overfit_batches=10,\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "model = BasicUNetLightning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                 | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model | BasicUNet            | 2.0 M  | train\n",
      "1 | loss  | DiceCELoss           | 0      | train\n",
      "2 | Dice  | GeneralizedDiceScore | 0      | train\n",
      "-------------------------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "7.914     Total estimated model params size (MB)\n",
      "144       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cd63414c1049faba9b347e772d7bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54812177e5b4928bdad256202e68904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c913943e94e439c8d470c24346fa2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6df0809cc0f4545a5116f7978cb71a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647efe302dcf4357a3d2b02cd796bc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55245ccdbeca4cd2a43b95572c7b2c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829a05fcc6a0405d85c502e161d06a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa1fc9a860b4b0d8134f4f2000ac0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b95518228e94ce297b0eb62d37f29bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608a88b3d9dc47399ede986d20688940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1db37950858439d8cb1537afa681601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c4db552b7f44c7a2b820203a6723ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6e2c90c93546398eea3d9b71427718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0822ed1f025540819e285b94168aa10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d61545046e446b83196b9e7b23ea5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d458a51e5c09401e9ada41a58b65f8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f23818f1e3146c89622811e22ca83d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0518af8735934e90b60f31b4400fc048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0542043117b4c03b4f10f2e5d6ab400",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5336c00eb43340e6b4baf1196e6020fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33273d4c4b184d1c84868f1e928c7d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7fbe3688ee60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/logging/__init__.py\", line 228, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913364486b014517995c0fb766e181d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea69a220ef7c4325af4bb75caa1df04e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py:225\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    223\u001b[0m     batch \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch, dataloader_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_progress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_batch_start(batch)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/loops/progress.py:142\u001b[0m, in \u001b[0;36m_Progress.increment_ready\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `total` and `current` instances should be of the same class\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mincrement_ready\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\u001b[38;5;241m.\u001b[39mready \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(model, data_module)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)\n",
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6699eef49ac048d4bf74a02578505487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/pytorch_lightning/loops/prediction_loop.py:255: predict returned None if it was on purpose, ignore this warning...\n"
     ]
    }
   ],
   "source": [
    "test_loader = data_module.test_dataloader()\n",
    "prediction = trainer.predict(model, dataloaders=test_loader)\n",
    "# len(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
