{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 72489,
          "databundleVersionId": 8096274,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30683,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **import from Kaggle**"
      ],
      "metadata": {
        "id": "OOMyK1-2-wtk"
      }
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'playground-series-s4e4:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F72489%2F8096274%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240414%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240414T161239Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D3a466f9f2bcea7b05fdcd142093ea8a60561b12f77110e73e701a3f6e513d2449087ce1dfc5d340dc96466ae8b9c54e465b8dcb6316e95736078759cea48d60da5bb74df01b66604b755eaf40e962dbaab06fbb2f9e93b6b533eb4f2b2103eddea99ab46ce1a16d95215543ef5290c5fe23c75d0520f782ecbf258924fff86b69d174008252a173ebe2b0adeb9551b703dc8e828b788c68de8cde7ba5eb0497bbda8985b44e78816fa518b4f94b96b1cccddc881126026b63746ae565c7eff91e378baa21b96f716fb4ca6c87ac140a5895ef6368980b1176244a5f19431015c29711f034c6a196ea60fd5939b951a5fbbab494d9f808c230dc12551fffac334'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "9pO8y0qgYdhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c628889b-d64d-47e1-80c7-17ce66a003c9"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading playground-series-s4e4, 2529839 bytes compressed\n",
            "[==================================================] 2529839 bytes downloaded\n",
            "Downloaded and uncompressed: playground-series-s4e4\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **import packs**"
      ],
      "metadata": {
        "id": "Y-GzhBUo-5ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning qqq\n",
        "!pip install segmentation-models-pytorch qqq\n",
        "!pip install wandb qqq\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_lightning as pl\n",
        "import segmentation_models_pytorch as smp\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "_uuid": "987c4cb8-9035-497f-b208-cc779b72a2dd",
        "_cell_guid": "699951bf-ecbb-443e-9c82-3f7ea79b941e",
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "ioO0VOOHYdhS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11310f60-04eb-44db-92d4-9b3c38b78a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.2.2-py3-none-any.whl (801 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qqq\n",
            "  Downloading qqq-0.0.1-py3-none-any.whl (1.3 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.11.0)\n",
            "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->pytorch-lightning) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->pytorch-lightning)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: qqq, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.2.2 qqq-0.0.1 torchmetrics-1.3.2\n",
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: qqq in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.17.1+cu121)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.9.2 (from segmentation-models-pytorch)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.1+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=976947b5dcb6e8293fa55f02d546aaeb5c5388378228b8e8fdf22e43e4c8e865\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=370b4ee97fdec458d65132364d0bb8ecbdb59c6fccc4d6a7328312f177c649c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3 timm-0.9.2\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: qqq in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.45.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **data preprocss**"
      ],
      "metadata": {
        "id": "kYXKcvyK_dup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training dataset\n",
        "train_data_path = \"/kaggle/input/playground-series-s4e4/train.csv\"\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "\n",
        "# Display the first 5 rows of the training dataset\n",
        "print(train_data.head())\n",
        "print('#################################################################')\n",
        "\n",
        "# Get basic information about the dataset, including data types and non-null values for each column\n",
        "print(train_data.info())\n",
        "print('#################################################################')\n",
        "\n",
        "# Perform exploratory data analysis to understand the distribution of the data\n",
        "print(train_data.describe())\n",
        "print('#################################################################')\n"
      ],
      "metadata": {
        "_uuid": "fbc863d9-8c52-4df9-8981-4393fb32aea3",
        "_cell_guid": "9cfadee4-8c6f-4910-a0f9-16ac83c2189a",
        "execution": {
          "iopub.status.busy": "2024-04-14T15:51:18.565551Z",
          "iopub.execute_input": "2024-04-14T15:51:18.566173Z",
          "iopub.status.idle": "2024-04-14T15:51:18.925017Z",
          "shell.execute_reply.started": "2024-04-14T15:51:18.566117Z",
          "shell.execute_reply": "2024-04-14T15:51:18.92372Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "2C-tJ-CIYdhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "data = pd.read_csv('/kaggle/input/playground-series-s4e4/train.csv')\n",
        "\n",
        "# Display basic information about the data\n",
        "print(\"Initial Data Information:\")\n",
        "print(data.info())\n",
        "print('#################################################################')\n",
        "\n",
        "# 1. Check and handle records with height 0\n",
        "zero_height = data[data['Height'] == 0]\n",
        "print(\"Number of records with height 0:\", zero_height.shape[0])\n",
        "print('#################################################################')\n",
        "\n",
        "# You can choose to delete these rows or replace them with mean/median\n",
        "data = data[data['Height'] > 0]  # Delete these rows\n",
        "# or\n",
        "# median_height = data['Height'].median()\n",
        "# data.loc[data['Height'] == 0, 'Height'] = median_height  # Replace with median\n",
        "\n",
        "# 2. Encode categorical variables and ensure they are floating-point\n",
        "data['Sex'] = data['Sex'].map({'M': 0.0, 'F': 1.0, 'I': 2.0})  # Label encoding to float\n",
        "# Convert data type to float\n",
        "data['Sex'] = data['Sex'].astype(float)\n",
        "\n",
        "# 3. Data normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define columns to be normalized\n",
        "features_to_scale = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Whole weight.1', 'Whole weight.2', 'Shell weight']\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Normalize these columns\n",
        "data[features_to_scale] = scaler.fit_transform(data[features_to_scale])\n",
        "\n",
        "# Display a sample of the normalized data\n",
        "print(\"Sample of normalized data:\")\n",
        "print(data[features_to_scale].head())\n",
        "\n",
        "# Save the cleaned data to the specified folder\n",
        "data.to_csv('/kaggle/working/cleaned_data.csv', index=False)\n",
        "print(\"Data cleaning completed and saved to '/kaggle/working/cleaned_data.csv'.\")\n",
        "\n",
        "print(\"Data cleaning completed and saved to cleaned_data.csv\")\n",
        "print('#################################################################')\n",
        "\n",
        "# Display basic information about the cleaned data\n",
        "print(\"Cleaned Data Information:\")\n",
        "print(data.info())\n",
        "print('#################################################################')\n",
        "\n",
        "# Display the first 5 rows of the cleaned dataset\n",
        "print(\"First 5 rows of cleaned dataset:\")\n",
        "print(data.head())\n",
        "print('#################################################################')\n"
      ],
      "metadata": {
        "_uuid": "79525daa-444d-4fc5-a023-7146f8a65d9b",
        "_cell_guid": "9eaee443-38bd-4140-b735-7e29203d6f6c",
        "execution": {
          "iopub.status.busy": "2024-04-14T15:52:07.769926Z",
          "iopub.execute_input": "2024-04-14T15:52:07.770432Z",
          "iopub.status.idle": "2024-04-14T15:52:09.848933Z",
          "shell.execute_reply.started": "2024-04-14T15:52:07.770387Z",
          "shell.execute_reply": "2024-04-14T15:52:09.847513Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "6rn5lVoVYdhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic information about the data\n",
        "print(data.info())  # Now we have reduced outliers and normalized the data\n",
        "print('#################################################################')\n",
        "\n",
        "# Display the first 5 rows of the cleaned dataset\n",
        "print(\"First 5 rows of cleaned dataset:\")\n",
        "print(data.head())\n",
        "print('#################################################################')\n"
      ],
      "metadata": {
        "_uuid": "b1657efe-413b-4d7a-8082-43c88b1e10f5",
        "_cell_guid": "31849d58-a2bc-419e-8df3-99fd56e86a58",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-04-14T15:53:55.02757Z",
          "iopub.execute_input": "2024-04-14T15:53:55.028122Z",
          "iopub.status.idle": "2024-04-14T15:53:55.052483Z",
          "shell.execute_reply.started": "2024-04-14T15:53:55.028082Z",
          "shell.execute_reply": "2024-04-14T15:53:55.05101Z"
        },
        "trusted": true,
        "id": "jXmpFH3dYdhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AbaloneDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom dataset class for the Abalone dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataframe):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with the provided DataFrame.\n",
        "\n",
        "        Args:\n",
        "            dataframe (pandas.DataFrame): The input DataFrame containing the Abalone data.\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the length of the dataset (number of samples).\n",
        "\n",
        "        Returns:\n",
        "            int: The length of the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def expand_and_fill(self, data):\n",
        "        \"\"\"\n",
        "        Expand and fill the input data into a 32x32 single-channel matrix.\n",
        "\n",
        "        Args:\n",
        "            data (torch.Tensor): The input data with shape [feature_size], where feature_size=8.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The expanded and filled data with shape [32, 32].\n",
        "        \"\"\"\n",
        "        # Initialize a 32x32 output matrix\n",
        "        output = torch.zeros((32, 32))\n",
        "\n",
        "        # Fill each 4 rows with one feature\n",
        "        for i in range(8):\n",
        "            output[i * 4:(i + 1) * 4, :] = data[i]\n",
        "\n",
        "        return output.unsqueeze(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single data sample and its corresponding target value.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The index of the sample in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the data (torch.Tensor) and target (torch.Tensor).\n",
        "        \"\"\"\n",
        "        # Extract all features except 'id' and 'Rings'\n",
        "        data = torch.tensor(\n",
        "            self.dataframe.iloc[idx][1:-1].values.astype(np.float32)\n",
        "        )  # Exclude id and Rings\n",
        "\n",
        "        # Expand and fill the data into a 32x32 matrix\n",
        "        data = self.expand_and_fill(data)\n",
        "\n",
        "        # Get the target value (Rings)\n",
        "        target = torch.tensor(self.dataframe.iloc[idx][-1], dtype=torch.float32)\n",
        "\n",
        "        return data, target\n",
        "\n",
        "\n",
        "# Load the data\n",
        "dataframe = pd.read_csv('/kaggle/working/cleaned_data.csv')\n",
        "\n",
        "# Create the dataset\n",
        "dataset = AbaloneDataset(dataframe)\n",
        "\n",
        "# Determine the size of the training and validation sets\n",
        "val_size = int(0.2 * len(dataset))  # Validation set size is 20% of the dataset\n",
        "train_size = len(dataset) - val_size  # Training set size is the remaining portion\n",
        "\n",
        "# Randomly split the dataset\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Check the output of the data loaders\n",
        "for data, target in train_loader:\n",
        "    print(\"Train batch - Data shape:\", data.shape, \"; Target shape:\", target.shape)\n",
        "    break\n",
        "\n",
        "for data, target in val_loader:\n",
        "    print(\"Validation batch - Data shape:\", data.shape, \"; Target shape:\", target.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "_uuid": "3c1c3d20-5321-4a11-a21e-422d0a040b11",
        "_cell_guid": "445d5fbe-a27c-45c0-a058-aee40609704f",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-04-14T16:04:59.979298Z",
          "iopub.execute_input": "2024-04-14T16:04:59.979791Z",
          "iopub.status.idle": "2024-04-14T16:05:01.086727Z",
          "shell.execute_reply.started": "2024-04-14T16:04:59.979752Z",
          "shell.execute_reply": "2024-04-14T16:05:01.085433Z"
        },
        "trusted": true,
        "id": "ghiq9IYhYdhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first batch of data from the train_loader\n",
        "data_iter = iter(train_loader)\n",
        "data_batch = next(data_iter)\n",
        "\n",
        "# Unpack the batch data\n",
        "inputs, targets = data_batch\n",
        "\n",
        "# Print the shapes and some data points to confirm correct data loading\n",
        "print(\"Input batch shape:\", inputs.shape)\n",
        "print(\"Target batch shape:\", targets.shape)\n",
        "print(\"First few inputs:\", inputs[0])\n",
        "print(\"First few targets:\", targets[0])\n",
        "\n",
        "# Select a sample input to visualize\n",
        "data_to_show = inputs[0]\n",
        "\n",
        "# Set up matplotlib plot\n",
        "fig, axes = plt.subplots(nrows=1, ncols=8, figsize=(20, 2.5))  # Create a 1x8 grid\n",
        "\n",
        "# Plot the image\n",
        "axes[0].imshow(data_to_show[0], cmap='gray')\n",
        "axes[0].set_title('Image')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "_uuid": "2cb93293-cb0f-4908-aa40-33feea9ef880",
        "_cell_guid": "42caba78-1715-43e4-b17b-18d8a64100a8",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-04-14T16:06:50.092863Z",
          "iopub.execute_input": "2024-04-14T16:06:50.093462Z",
          "iopub.status.idle": "2024-04-14T16:06:50.42875Z",
          "shell.execute_reply.started": "2024-04-14T16:06:50.093408Z",
          "shell.execute_reply": "2024-04-14T16:06:50.426171Z"
        },
        "trusted": true,
        "id": "HTR8lS9xYdhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmsle(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Calculate the Root Mean Squared Logarithmic Error (RMSLE).\n",
        "\n",
        "    Args:\n",
        "        y_pred (torch.Tensor): The predicted outputs from the model, should be a tensor.\n",
        "        y_true (torch.Tensor): The true target values, should be a tensor of the same shape as y_pred.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A scalar tensor representing the RMSLE for the current batch.\n",
        "    \"\"\"\n",
        "    # Ensure predictions and targets are greater than zero after adding one to avoid log(0)\n",
        "    log_pred = torch.log1p(y_pred)\n",
        "    log_true = torch.log1p(y_true)\n",
        "\n",
        "    # Calculate the squared differences between the two\n",
        "    squared_log_error = (log_pred - log_true) ** 2\n",
        "\n",
        "    # Mean the squared differences then take the square root\n",
        "    mean_squared_log_error = torch.mean(squared_log_error)\n",
        "    rmsle = torch.sqrt(mean_squared_log_error)\n",
        "\n",
        "    return rmsle\n",
        "\n",
        "# Test it with a simple tensor 😝\n",
        "y_pred = torch.tensor([3.0, 5.0, 2.5], dtype=torch.float32)\n",
        "y_true = torch.tensor([2.0, 4.0, 3.0], dtype=torch.float32)\n",
        "\n",
        "# Calculate the RMSLE\n",
        "loss = rmsle(y_pred, y_true)\n",
        "print(\"RMSLE Loss:\", loss.item())\n"
      ],
      "metadata": {
        "_uuid": "31b8ca11-b74f-447e-9d0e-dfa6ec12c4f7",
        "_cell_guid": "7b968778-193f-4034-a4d2-f7f8a83d50e9",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-04-14T16:07:24.200705Z",
          "iopub.execute_input": "2024-04-14T16:07:24.201284Z",
          "iopub.status.idle": "2024-04-14T16:07:24.212798Z",
          "shell.execute_reply.started": "2024-04-14T16:07:24.201231Z",
          "shell.execute_reply": "2024-04-14T16:07:24.210912Z"
        },
        "trusted": true,
        "id": "a0lir7jfYdhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AbaloneModel(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    U-Net based regression model for Abalone age prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AbaloneModel, self).__init__()\n",
        "\n",
        "        # Encoder-Decoder architecture using U-Net\n",
        "        self.model = smp.Unet(\n",
        "            encoder_name=\"mit_b1\",  # Select encoder (e.g., ResNet34)\n",
        "            # encoder_weights=\"imagenet\",  # Use pretrained weights\n",
        "            in_channels=3,\n",
        "            classes=1,  # Output a single age prediction\n",
        "            decoder_attention_type='scse',  # No activation function or whatever you like\n",
        "        )\n",
        "\n",
        "        # Regressor head to output age prediction\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),  # Pool to 1x1\n",
        "            nn.Flatten(),  # Flatten\n",
        "            nn.Linear(1, 1),  # Fully connected layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Repeat input channel to match U-Net input (3 channels)\n",
        "        x = x.repeat(1, 3, 1, 1)\n",
        "\n",
        "        # Pass through U-Net encoder-decoder\n",
        "        x = self.model(x)\n",
        "\n",
        "        # Apply regressor head for age prediction\n",
        "        x = self.regressor(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)  # Model output should be [batch_size, 1]\n",
        "        y_hat = y_hat.squeeze(-1)  # Remove last dim, shape becomes [batch_size]\n",
        "\n",
        "        loss = rmsle(y_hat, y)\n",
        "\n",
        "        # Log training loss\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=False, logger=True, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)  # Model predictions on validation set\n",
        "        y_hat = y_hat.squeeze(-1)  # Ensure predictions are [batch_size]\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = rmsle(y_hat, y)\n",
        "\n",
        "        # Optionally: Log validation loss\n",
        "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, logger=True, prog_bar=True)\n",
        "\n",
        "        # return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(self.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "_uuid": "4b89445d-b3d5-45e3-a18d-0ef6cfb63779",
        "_cell_guid": "34fda8f4-6aca-46fe-a1c5-11fca6f055d7",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-04-14T16:09:10.175602Z",
          "iopub.execute_input": "2024-04-14T16:09:10.176199Z",
          "iopub.status.idle": "2024-04-14T16:09:10.192364Z",
          "shell.execute_reply.started": "2024-04-14T16:09:10.176156Z",
          "shell.execute_reply": "2024-04-14T16:09:10.19071Z"
        },
        "trusted": true,
        "id": "W9Qa-kLxYdhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb_logger = WandbLogger(project=\"Abalone\", name=\"Round4\")\n",
        "\n",
        "model = AbaloneModel()\n",
        "trainer = pl.Trainer(max_epochs=40, logger=None)\n",
        "\n",
        "# Go train that shxt\n",
        "trainer.fit(model, train_loader, val_loader)"
      ],
      "metadata": {
        "_uuid": "4022d328-5b81-49e9-be7f-88bad4c7a420",
        "_cell_guid": "5f2f14b4-c49b-4923-888b-781d923a75d8",
        "execution": {
          "iopub.status.busy": "2024-04-11T03:09:42.712011Z",
          "iopub.execute_input": "2024-04-11T03:09:42.712747Z",
          "iopub.status.idle": "2024-04-11T03:10:25.304009Z",
          "shell.execute_reply.started": "2024-04-11T03:09:42.712699Z",
          "shell.execute_reply": "2024-04-11T03:10:25.302914Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "pWPjmRndYdhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "IIMhLveagO1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WTF IT WORKS?**😝😝😝😝😝😝😝😝😝"
      ],
      "metadata": {
        "_uuid": "a04895f1-33cc-4f87-a309-697a1b40dbca",
        "_cell_guid": "5e57f59c-553e-4f5a-9d02-831e5b08ae6a",
        "trusted": true,
        "id": "6g-tNZPyYdhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AbaloneModel.load_from_checkpoint(\"/content/drive/MyDrive/epoch=13-step=15862.ckpt\")"
      ],
      "metadata": {
        "id": "1dZE_7Jn9x-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "model.freeze()  # In PyTorch Lightning, freeze to ensure model parameters don't change\n",
        "\n",
        "# Create a DataFrame to store results\n",
        "results = pd.DataFrame()\n",
        "\n",
        "# Disable gradient computation for inference\n",
        "@torch.no_grad()\n",
        "for batch in val_loader:  # Assume val_loader is your validation data loader\n",
        "    inputs, targets = batch\n",
        "    # inputs = inputs.to('cuda')  # Move input data to GPU\n",
        "\n",
        "    predictions = model(inputs)  # Get model predictions\n",
        "    predictions = predictions.squeeze(-1)  # Adjust prediction shape if needed\n",
        "\n",
        "    # Convert data to CPU and NumPy\n",
        "    predictions = predictions.cpu().numpy()\n",
        "    targets = targets.cpu().numpy()\n",
        "\n",
        "    # Add results to DataFrame\n",
        "    batch_results = pd.DataFrame({\n",
        "        \"Predicted Age\": predictions,\n",
        "        \"Actual Age\": targets\n",
        "    })\n",
        "    results = pd.concat([results, batch_results], ignore_index=True)\n",
        "\n",
        "# Display or analyze results\n",
        "print(results.head())\n",
        "\n",
        "# Save results to CSV (optional)\n",
        "results.to_csv(\"prediction_results.csv\", index=False)\n"
      ],
      "metadata": {
        "_uuid": "df7cc063-7071-49f1-ab82-ae7d3f47808c",
        "_cell_guid": "49b0f538-b5a1-4539-aeb5-f30609139d01",
        "execution": {
          "iopub.status.busy": "2024-04-10T17:51:39.003104Z",
          "iopub.execute_input": "2024-04-10T17:51:39.003908Z",
          "iopub.status.idle": "2024-04-10T17:51:39.06173Z",
          "shell.execute_reply.started": "2024-04-10T17:51:39.003874Z",
          "shell.execute_reply": "2024-04-10T17:51:39.060809Z"
        },
        "jupyter": {
          "outputs_hidden": false
        },
        "trusted": true,
        "id": "jocoszhNYdhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 读取 CSV 文件\n",
        "df = pd.read_csv(\"/content/prediction_results.csv\")\n",
        "\n",
        "# 计算预测年龄与实际年龄的差\n",
        "age_diff = df[\"Predicted Age\"] - df[\"Actual Age\"]\n",
        "\n",
        "# 将差值绝对值小于 1 的视为准确预测\n",
        "accurate_predictions = (age_diff.abs() < 1).astype(int)\n",
        "\n",
        "# 计算准确率\n",
        "accuracy = accurate_predictions.mean()\n",
        "\n",
        "# 打印准确率\n",
        "print(\"准确率:\", accuracy)"
      ],
      "metadata": {
        "id": "ADJL-uopCfWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# 确保模型处于评估模式\n",
        "model.eval()\n",
        "model.freeze()  # 在 PyTorch Lightning 中，冻结用于确保模型参数不会改变\n",
        "\n",
        "# 创建一个 DataFrame 存储结果\n",
        "results = pd.DataFrame()\n",
        "\n",
        "# 用于计算整体准确率\n",
        "accurate_predictions_count = 0\n",
        "total_predictions_count = 0\n",
        "\n",
        "# 关闭梯度计算，用于推理\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:  # 假设 val_loader 是你的验证集加载器\n",
        "        inputs, targets = batch\n",
        "        # inputs = inputs.to('cuda')  # 将输入数据移至 GPU\n",
        "\n",
        "        predictions = model(inputs)  # 获取模型的预测结果\n",
        "        predictions = predictions.squeeze(-1)  # 调整预测结果的形状，如果需要的话\n",
        "\n",
        "        # 将数据转换为 CPU 并转换为 NumPy\n",
        "        predictions = predictions.cpu().numpy()\n",
        "        targets = targets.cpu().numpy()\n",
        "\n",
        "        # 计算准确率：预测和实际年龄相差1岁以内\n",
        "        accurate_predictions = np.abs(predictions - targets) <= 1\n",
        "        accurate_predictions_count += np.sum(accurate_predictions)\n",
        "        total_predictions_count += len(predictions)\n",
        "\n",
        "        # 将结果添加到 DataFrame\n",
        "        batch_results = pd.DataFrame({\n",
        "            \"Predicted Age\": predictions,\n",
        "            \"Actual Age\": targets,\n",
        "            \"Accurate\": accurate_predictions\n",
        "        })\n",
        "        results = pd.concat([results, batch_results], ignore_index=True)\n",
        "\n",
        "# 计算整体准确率\n",
        "overall_accuracy = accurate_predictions_count / total_predictions_count * 100\n",
        "print(f\"Overall Accuracy: {overall_accuracy:.2f}%\")\n",
        "\n",
        "# 显示或分析结果\n",
        "print(results.head())\n",
        "\n",
        "# 可以保存结果到 CSV\n",
        "results.to_csv(\"prediction_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "qguj2KFHA2ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: /content/Abalone/46gjhnho/checkpoints/epoch=13-step=15862.ckpt copied to /content/drive/MyDrive\n",
        "\n",
        "!cp /content/Abalone/46gjhnho/checkpoints/epoch=13-step=15862.ckpt /content/drive/MyDrive\n"
      ],
      "metadata": {
        "id": "MbpdHT0OvykA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-5APYdjEv1yu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}